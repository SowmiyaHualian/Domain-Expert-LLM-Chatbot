# ğŸ§  Domain Expert LLM Chatbot

A full-stack AI chatbot designed to act as a Machine Learning domain expert.  
The system integrates a real LLM API with a structured FastAPI backend and a responsive frontend interface.

---

## ğŸš€ Overview

The Domain Expert LLM Chatbot is a session-aware AI assistant that:

- Answers Machine Learning theory and project-related questions
- Maintains short-term conversational context
- Uses prompt engineering (no fine-tuning)
- Integrates with a live LLM API (Groq)
- Implements a clean full-stack architecture

This project demonstrates backend design, API integration, session handling, and UI system design.

---

## ğŸ—ï¸ System Architecture

Frontend (HTML/CSS/JavaScript)  
â¬‡  
FastAPI Backend (`/chat` REST endpoint)  
â¬‡  
Prompt Engineering Layer  
â¬‡  
Groq LLM API (Llama 3.1 Instant)  

---

## ğŸ“‚ Project Structure

```
domain-expert-llm-chatbot/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ prompt.py
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ script.js
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§  Key Features

### âœ… Prompt-Engineered ML Expert
- System prompt enforces domain restriction
- Structured explanations
- Focus on correctness and clarity

### âœ… Session-Based Memory
- Cookie-based session ID
- Maintains last 10 message exchanges
- Prevents cross-user conversation leakage

### âœ… Full-Stack Integration
- REST API design
- Asynchronous frontend fetch
- JSON request/response handling

### âœ… Clean UI Implementation
- Structured layout
- Dynamic message rendering
- Typing indicator
- Clear chat functionality

---

## âš™ï¸ Technologies Used

### Backend
- Python
- FastAPI
- Requests
- Pydantic

### Frontend
- HTML5
- CSS3
- Vanilla JavaScript

### LLM Provider
- Groq API
- Model: `llama-3.1-8b-instant`

---

## ğŸ” Environment Setup

Create a `.env` file inside the `backend/` folder:

```
LLM_API_KEY=your_groq_key_here
LLM_API_URL=https://api.groq.com/openai/v1/chat/completions
```

---

## â–¶ï¸ Running the Project

### 1ï¸âƒ£ Install Dependencies

```
pip install fastapi uvicorn python-dotenv requests
```

### 2ï¸âƒ£ Start Backend

From project root:

```
uvicorn backend.main:app --reload
```

### 3ï¸âƒ£ Open Frontend

Open:

```
frontend/index.html
```

---

## ğŸ“Œ Example Use Cases

- Explaining bias-variance tradeoff
- Comparing optimization algorithms
- Discussing regularization methods
- Reviewing ML project approaches
- Clarifying exam-related ML concepts

---

## ğŸ§© Design Decisions

- No fine-tuning â†’ Prompt engineering only
- No database â†’ In-memory session store (MVP)
- Context trimming â†’ Last 10 messages only
- Clean modular structure for scalability

---

## ğŸ“¸ Screenshots

### ğŸ–¥ï¸ Project Structure

![structure](./screenshots/structure.png)

### ğŸ’¬ Chat Interaction Example

![Chat Example](./screenshots/chat-example.png)
![Chat Example(1)](./screenshots/chat-example(1).png)

---


## ğŸ“Š Architecture Diagram

```
User (Browser)
      â”‚
      â–¼
Frontend (HTML/CSS/JS)
      â”‚
      â–¼
FastAPI Backend (/chat endpoint)
      â”‚
      â–¼
Prompt Engineering Layer
      â”‚
      â–¼
Groq LLM API (llama-3.1-8b-instant)
```

---
## ğŸš€ Future Improvements

- Persistent database-backed memory
- Authentication system
- Markdown rendering
- Multi-domain expert modes
- Deployment to cloud platform

---

## ğŸ“„ License

This project is for educational and portfolio purposes.

